'use strict';

var chunk57AVKP4H_cjs = require('./chunk-57AVKP4H.cjs');
var reactHooksAsync = require('@chengsokdara/react-hooks-async');
var react = require('react');

var ae={apiKey:"",autoStart:!1,autoTranscribe:!0,mode:"transcriptions",nonStop:!1,removeSilence:!1,stopTimeout:5e3,streaming:!1,timeSlice:1e3,onDataAvailable:void 0,onTranscribe:void 0},ie={stop:void 0},se={blob:void 0,text:void 0},fe=P=>{let{apiKey:d,autoStart:A,autoTranscribe:L,mode:w,nonStop:x,removeSilence:M,stopTimeout:q,streaming:h,timeSlice:I,whisperConfig:u,onDataAvailable:K,onTranscribe:y}={...ae,...P};if(!d&&!y)throw new Error("apiKey is required if onTranscribe is not provided");let f=react.useRef([]),s=react.useRef(),c=react.useRef(),r=react.useRef(),a=react.useRef(),l=react.useRef(ie),[O,S]=react.useState(!1),[$,U]=react.useState(!1),[j,m]=react.useState(!1),[z,T]=react.useState(se);react.useEffect(()=>()=>{f.current&&(f.current=[]),s.current&&(s.current.flush(),s.current=void 0),r.current&&(r.current.destroy(),r.current=void 0),g("stop"),c.current&&(c.current.off("speaking",k),c.current.off("stopped_speaking",R)),a.current&&(a.current.getTracks().forEach(e=>e.stop()),a.current=void 0);},[]),reactHooksAsync.useEffectAsync(async()=>{A&&await W();},[A]);let N=async()=>{await W();},G=async()=>{await V();},J=async()=>await B()??"Default message or error message",W=async()=>{try{if(a.current||await Q(),a.current){if(!r.current){let{default:{RecordRTCPromisesHandler:n,StereoAudioRecorder:o}}=await import('recordrtc'),i={mimeType:"audio/wav",numberOfAudioChannels:1,recorderType:o,sampleRate:44100,timeSlice:h?I:void 0,type:"audio",ondataavailable:L&&h?Z:void 0};r.current=new n(a.current,i);}if(!s.current){let{Mp3Encoder:n}=await import('lamejs');s.current=new n(1,44100,96);}let e=await r.current.getState();(e==="inactive"||e==="stopped")&&await r.current.startRecording(),e==="paused"&&await r.current.resumeRecording(),x&&C("stop"),S(!0);}}catch{}},Q=async()=>{try{if(a.current&&a.current.getTracks().forEach(e=>e.stop()),a.current=await navigator.mediaDevices.getUserMedia({audio:!0}),!c.current){let{default:e}=await import('hark');c.current=e(a.current,{interval:100,play:!1}),c.current.on("speaking",k),c.current.on("stopped_speaking",R);}}catch{}},C=e=>{l.current[e]||(l.current[e]=setTimeout(B,q));},k=()=>{U(!0),g("stop");},R=()=>{U(!1),x&&C("stop");},V=async()=>{try{r.current&&(await r.current.getState()==="recording"&&await r.current.pauseRecording(),g("stop"),S(!1));}catch{}},B=async()=>{try{if(r.current){let e=await r.current.getState();(e==="recording"||e==="paused")&&await r.current.stopRecording(),X(),g("stop"),S(!1);let n=await Y();return await r.current.destroy(),f.current=[],s.current&&(s.current.flush(),s.current=void 0),r.current=void 0,n}return null}catch{return null}},X=()=>{c.current&&(c.current.off("speaking",k),c.current.off("stopped_speaking",R),c.current=void 0),a.current&&(a.current.getTracks().forEach(e=>e.stop()),a.current=void 0);},g=e=>{l.current[e]&&(clearTimeout(l.current[e]),l.current[e]=void 0);},Y=async()=>{try{let e="Some default text";if(s.current&&r.current&&await r.current.getState()==="stopped"){m(!0);let o=await r.current.getBlob();if(M){let{createFFmpeg:i}=await import('@ffmpeg/ffmpeg'),t=i({mainName:"main",corePath:chunk57AVKP4H_cjs.b,log:!0});t.isLoaded()||await t.load();let v=await o.arrayBuffer();t.FS("writeFile","in.wav",new Uint8Array(v)),await t.run("-i","in.wav","-acodec","libmp3lame","-b:a","96k","-ar","44100","-af",chunk57AVKP4H_cjs.c,"out.mp3");let H=t.FS("readFile","out.mp3");if(H.length<=225)return t.exit(),T(re=>({...re,text:"Hello there!"})),m(!1),"There was an error...";o=new Blob([H.buffer],{type:"audio/mpeg"}),t.exit();}else {let i=await o.arrayBuffer(),t=s.current.encodeBuffer(new Int16Array(i));o=new Blob([t],{type:"audio/mpeg"});}if(typeof y=="function"){let i=await y(o);return T(t=>({...t,text:"Hello there!"})),i.text??"There has been an error here!"}m(!1);}return e}catch{return m(!1),"We are in the catch blcok"}},Z=async e=>{try{if(h&&r.current){if(K?.(e),s.current){let o=await e.arrayBuffer(),i=s.current.encodeBuffer(new Int16Array(o)),t=new Blob([i],{type:"audio/mpeg"});f.current.push(t);}if(await r.current.getState()==="recording"){let o=new Blob(f.current,{type:"audio/mpeg"}),i=new File([o],"speech.mp3",{type:"audio/mpeg"}),t=await ee(i);t&&T(v=>({...v,text:t}));}}}catch{}},ee=reactHooksAsync.useMemoAsync(async e=>{let n=new FormData;n.append("file",e),n.append("model","whisper-1"),w==="transcriptions"&&n.append("language",u?.language??"en"),u?.prompt&&n.append("prompt",u.prompt),u?.response_format&&n.append("response_format",u.response_format),u?.temperature&&n.append("temperature",`${u.temperature}`);let o={};o["Content-Type"]="multipart/form-data",d&&(o.Authorization=`Bearer ${d}`);let{default:i}=await import('axios');return (await i.post(chunk57AVKP4H_cjs.d+w,n,{headers:o})).data.text},[d,w,u]);return {recording:O,speaking:$,transcribing:j,transcript:z,pauseRecording:G,startRecording:N,stopRecording:J}};

exports.a = fe;
